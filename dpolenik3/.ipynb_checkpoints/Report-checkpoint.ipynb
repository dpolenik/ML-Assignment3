{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and Dimensional Reduction\n",
    "## The Data \n",
    "I decided to use the wine quality dataset and the Beer dataset I used in my first assignment \n",
    "https://archive.ics.uci.edu/ml/machinelearningdatabases/wine-quality/winequality.names. A quick recap of the wine dataset. It contains 11 features and 10 output labels indicating the quality of the wine. \n",
    "\n",
    "https://www.kaggle.com/jtrofe/beer-recipes This is the beer dataset. It contained a number of features, 10 of which were usable in analysis. The other fields were just url locations for where the recipes came from. It has a high number of labels though. It has a total of 176.\n",
    "\n",
    "\n",
    "# K-Means\n",
    "\n",
    "## Beer Dataset\n",
    "I trained the beer dataset by creating an elbow graph below. We start to lose effectiveness around 7-10 clusters.\n",
    "##### Elbow and Clusters\n",
    "Elbow| Clusters\n",
    "- | - \n",
    "<img src=\".\\plots\\KMeans\\Beer-Elbow.png\"/>|<img src=\".\\plots\\KMeans\\beerClusters.png\"/>\n",
    "I trained the beer dataset by creating an elbow graph below. We start to lose effectiveness around 7-10 clusters.\n",
    "I used this analysis to decide on 10 clusters. The clusters look like they center around the outlier values and then I have several clusters around more densely populated areas. An example of this is the 30-40 IBU range. The BoilSize is another example of this because the centers are widely distributed. Unfortunately, the clusters did not line up well at all. I had 170+ labels and I knew that number of clusters wouldn't classify this well.\n",
    "## Wine Dataset\n",
    "For the wine dataset I used the elbow curve to decide on 8 clusters. The clusters again lined up well in terms of their distribution. If there were outliers, like in residual sugar, a cluster settled there. If there were more dense distribution areas, more clusters settle around them. These didn't match up with my labels.\n",
    "##### Elbow and Clusters\n",
    "Elbow| Clusters\n",
    "- | - |-\n",
    "<img src=\".\\plots\\KMeans\\Wine-Elbow.png\"/>|<img src=\".\\plots\\KMeans\\wineClusters.png\"/>\n",
    "\n",
    "# Expectation Maximization\n",
    "## Beer Dataset\n",
    "I used a BIC graph to determine the number of clusters I wanted for EM. I used an elbow graph to inform help inform that decision and find the point of diminishing returns for the number of clusters.\n",
    "##### Elbow and BIC\n",
    "Elbow| BIC\n",
    "- | - \n",
    "<img src=\".\\plots\\EM\\Beer_Elbow.png\" width=\"300\"/>|<img src=\".\\plots\\EM\\BIC_Beer.png\"  width=\"300\"/>\n",
    "<img src=\".\\plots\\EM\\beerClustersGMM.png\"  width=\"700\"/>\n",
    "Based on the combination of charts I chose 8 clusters. These clusters still didn't match up with my labels. There were too many labels in my original dataset. The centers were quite a bit different than Kmeans. The centers were very close together for serveral values.\n",
    "\n",
    "## Wine Dataset\n",
    "I used 6 clusters for the wine dataset. Like the beer dataset these clusters were closer together than the K-Means clusters. The clusters between K-Means and EM look very similar.\n",
    "##### Elbow and BIC\n",
    "Elbow| BIC\n",
    "- | - \n",
    "<img src=\".\\plots\\EM\\Wine_Elbow.png\"/>|<img src=\".\\plots\\EM\\BIC_Wine.png\"/>\n",
    "<img src=\".\\plots\\EM\\wineClustersGMM.png\" width=\"700\"/>\n",
    "\n",
    "# PCA\n",
    "## Beer Dataset\n",
    "To choose the number of components I used an Elbow Graph of the explained variance ratio. Shown below, at 5 components my ration is at 1 meaning all of my variance can be explained in 4 components. That reduces it from 9 features. I used this to create 5 principal components.\n",
    "<img src=\".\\plots\\PCA\\Beer_Elbow.png\"/>\n",
    "## Wine Dataset\n",
    "Similarly, I was able to reduce the number of features in the wine dataset from 11 to only 4. The first value explains most of the variance and the remaining 4 cover from ~99% to the remaining 100%.\n",
    "<img src=\".\\plots\\PCA\\Wine_Elbow.png\"/>\n",
    "\n",
    "# ICA \n",
    "## Beer Dataset\n",
    "For ICA, I used a kurtosis curve to decide the number of components. I took the maximum value of the curve below and decided to use 2 components. My maximum value was actually one, but I couldn't do anything interesting with only one component.\n",
    "<img src=\".\\plots\\ICA\\Kurtosis_Curve_Beer.png\"/>\n",
    "## Wine Dataset\n",
    "The wine dataset gave a better output. It peaked at 10 features. This didn't reduce my dimensions at all though.\n",
    "<img src=\".\\plots\\ICA\\Kurtosis_Curve_Wine.png\"/>\n",
    "## ICA Clustering\n",
    "### K-Means\n",
    "### Beer Dataset\n",
    "For K-Means I used an elbow curve to decide on the number of clusters. It seems to peak at 2 so there's not much to show here. Again K- means. The clusters seem to represent roughly the same amount with one cluster representing far more than the other four.\n",
    "#### Elbow and Pair Plot\n",
    "Elbow| Pairplot\n",
    "- | - \n",
    "<img src=\".\\plots\\ICA\\Elbow_Beer.png\"/>|<img src=\".\\plots\\ICA\\PairPlot_Beer_Kmeans.png\" width=\"600\"/>\n",
    "### Wine Dataset\n",
    "ICA didn't reduce the dimensionality on I chose 20 clusters based on the elbow curve but because the dimensionality wasn't reduced and there's many clusters. This section was difficult to interpret.\n",
    "\n",
    "<img src=\".\\plots\\ICA\\Elbow_Wine_Kmeans.png\"/>\n",
    "## PCA Clustering\n",
    "### K-Means\n",
    "### Beer Dataset\n",
    "After applying PCA the number of clusters was cut to 5. This also allowed for some better visualizations of the features. \n",
    "The pair plot shows a how the different components interact with each other. Component one seems to have the most visible separations in the data showing nice slices. This is much more readable than the original pair plots. There was simply too much data to understand visually. It still couldnâ€™t classify the labels and is getting further from it. We'd need the clusters to increase instead of decrease for this dataset. The centroids spread out further than the original. It gives clear lines in the visualizations especially for component one.\n",
    "\n",
    "#### Elbow and Pair Plot\n",
    "Elbow| Pairplot\n",
    "- | - \n",
    "<img src=\".\\plots\\PCA\\PCA_Kmeans_Elbow_Beer.png\"/>|<img src=\".\\plots\\PCA\\PCA_Kmeans_pairplot_Beer.png\" width=\"600\"/>\n",
    "<img src=\".\\plots\\PCA\\beerClusters_PCA.png\"/>\n",
    "### Wine Dataset\n",
    "Rerunning my wine quality dataset, I again needed reduce the number of clusters. This time only to three. The clusters do not match the labels well because there are 10 output labels. This does allow me to neatly chart my data again, but It seems like I lost information in this transformation.\n",
    "\n",
    "The centers are evenly spaced again. Each area seems to represent a high, medium and love value for each cluster.\n",
    "#### Elbow and Pair Plot\n",
    "Elbow| Pairplot\n",
    "- | - \n",
    "<img src=\".\\plots\\PCA\\PCA_Kmeans_Elbow_Wine.png\"/>|<img src=\".\\plots\\PCA\\PCA_Kmeans_pairplot_Wine.png\" width=\"600\"/>\n",
    "<img src=\".\\plots\\PCA\\wineClusters_PCA.png\" width=\"600\"/>\n",
    "### Expectation Maximization\n",
    "### Beer Dataset\n",
    "I got the same number of clusters for the beer dataset as I did for the original, 8. With the reduced dimensions it's clearer to see where the clusters are. It seems like the clusters line up with the combinations of components. This is different from K-Means in that they were clustered around component 1. These seem to be more evenly spaced between features.\n",
    "##### Elbow and BIC\n",
    "Elbow| BIC\n",
    "- | - \n",
    "<img src=\".\\plots\\PCA\\Elbow_PCA_Beer.png\" width=\"300\"/>|<img src=\".\\plots\\PCA\\BIC_PCA_Beer.png\"  width=\"300\"/>\n",
    "<img src=\".\\plots\\PCA\\PCA_EM_pairplot_Beer.png\"  width=\"600\"/>\n",
    "### Wine Dataset\n",
    "This one wasn't as clear to me. I Based on the two graphs I decided to go with 3 clusters. I leaned toward BIC mostly based on Piazza comments. Expectation Maximization again seemed to cluster more evenly between components. Where there were clear and defined lines in K-Means that were most apparent around component 1. EM seemed to have decent representation of clusters in all the between all the features. Rather than being grouped and pinned only by the centroid with no flexibility.\n",
    "##### Elbow and BIC\n",
    "Elbow| BIC\n",
    "- | - \n",
    "<img src=\".\\plots\\PCA\\Elbow_PCA_EM_Beer.png\" width=\"300\"/>|<img src=\".\\plots\\PCA\\BIC_PCA_EM_Beer.png\"  width=\"300\"/>\n",
    "\n",
    "# PCA Neural Network\n",
    "## Beer Dataset\n",
    "\n",
    "## Wine Dataset\n",
    "These results were also not as good as my original experiment. It seems like I lose a bit of information when I transform the features using PCA. I was expecting these to be a closer to my original experiment but this one lost roughly 6% accuracy compared to the raw value neural network. This was the case for both the single and two-layer networks.\n",
    "<img src=\".\\plots\\PCA\\PCA_NN_Wine.png\"/>\n",
    "### PCA & KMeans Neural Net \n",
    "This neural network performed better but is still suffering from the information loss in PCA. My original neural network was able to correctly classify 65% of the time on the two-layer network. This one at its best was only able to predict correctly 60% of the time.\n",
    "<img src=\".\\plots\\PCA\\PCA_Kmeans_NN_Wine.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
